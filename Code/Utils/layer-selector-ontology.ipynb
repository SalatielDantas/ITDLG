{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kupURl51fdTC"
   },
   "source": [
    "# Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndDCxpXXfXKK",
    "outputId": "1b68dc53-7b86-4675-8d92-1bb6a54c2e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Owlready2\n",
      "  Downloading Owlready2-0.35.tar.gz (23.8 MB)\n",
      "Using legacy 'setup.py install' for Owlready2, since package 'wheel' is not installed.\n",
      "Installing collected packages: Owlready2\n",
      "    Running setup.py install for Owlready2: started\n",
      "    Running setup.py install for Owlready2: finished with status 'done'\n",
      "Successfully installed Owlready2-0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 21.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tield\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install Owlready2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in ./anaconda3/lib/python3.8/site-packages (5.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.6 in ./anaconda3/lib/python3.8/site-packages (from pyarrow) (1.19.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastparquet in ./anaconda3/lib/python3.8/site-packages (0.7.1)\r\n",
      "Requirement already satisfied: numpy>=1.18 in ./anaconda3/lib/python3.8/site-packages (from fastparquet) (1.19.5)\r\n",
      "Requirement already satisfied: pandas>=1.1.0 in ./anaconda3/lib/python3.8/site-packages (from fastparquet) (1.2.4)\r\n",
      "Requirement already satisfied: thrift>=0.11.0 in ./anaconda3/lib/python3.8/site-packages (from fastparquet) (0.15.0)\r\n",
      "Requirement already satisfied: fsspec in ./anaconda3/lib/python3.8/site-packages (from fastparquet) (0.9.0)\r\n",
      "Requirement already satisfied: cramjam>=2.3.0 in ./anaconda3/lib/python3.8/site-packages (from fastparquet) (2.4.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./anaconda3/lib/python3.8/site-packages (from pandas>=1.1.0->fastparquet) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in ./anaconda3/lib/python3.8/site-packages (from pandas>=1.1.0->fastparquet) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->fastparquet) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uksZD_XnfhqN"
   },
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-zj0RvcgfjGk"
   },
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tbzhUrDyfV_"
   },
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-Q_iuC0-olK"
   },
   "source": [
    "## Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BfyGnkHZ-pxL"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "#Função para carregar descrições de amenities salvas em json\n",
    "def loadJSON(fileName):\n",
    "  with open(fileName) as json_file:\n",
    "    itemData = json.load(json_file)\n",
    "  return itemData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uexzoynN9lZk"
   },
   "source": [
    "## Geradoras de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "USMAPFWXwbbA"
   },
   "outputs": [],
   "source": [
    "def extractTuplesLayerByBin(n, wgt):\n",
    "\n",
    "  try:\n",
    "    print('Extracting tuples of bin:', n, '...')\n",
    "    #Step 0: Loading the ontology\n",
    "    onto_path.clear()\n",
    "    ontology = get_ontology(\"./yelp_ontology.owl\")\n",
    "    ontology.load()\n",
    "\n",
    "    #file_name = './Austin/w07/Complete bins/austin-sl-tuple-n-itdl-'+str(n)+'bin-wgt'+str(wgt)+'.csv'\n",
    "    #tuples = pd.read_csv(file_name)\n",
    "    \n",
    "    file_name = './Austin/w05/Partial bins/austin-sl-tuple-n-itdl-'+str(n)+'bin-wgt'+str(wgt)+'-p.parquet'\n",
    "    tuples = pd.read_parquet(file_name)\n",
    "    #tuples = tuples[['center_poi', 'context_poi', 'distance-m']]\n",
    "    \n",
    "    layers = [1, 2, 3]\n",
    "    for l in layers:\n",
    "\n",
    "      layer_tuples = getTuplesFromLayer(ontology, tuples, l)\n",
    "\n",
    "      #Se há dados\n",
    "      if(len(layer_tuples) > 0):\n",
    "        \n",
    "        #layer_tuples_file_name = './Austin/w07/Complete bins/austin-sl-tuple-n-itdl-'+str(n)+'bin-wgt'+str(wgt)+'-l'+str(l)+'.csv'          \n",
    "        #layer_tuples.to_csv(layer_tuples_file_name, index=False)\n",
    "        \n",
    "        layer_tuples_file_name = './Austin/w05/Partial bins/austin-sl-tuple-n-itdl-'+str(n)+'bin-wgt'+str(wgt)+'-l'+str(l)+'-p.parquet'          \n",
    "        layer_tuples.to_parquet(layer_tuples_file_name, compression='brotli',  index = False)\n",
    "                   \n",
    "  except Exception as e:\n",
    "    #e\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSentencesLayerByBin(n, wgt):\n",
    "\n",
    "  try:\n",
    "    print('Extracting sentences of bin:', n, '...')\n",
    "    #Step 0: Loading the ontology\n",
    "    onto_path.clear()\n",
    "    ontology = get_ontology(\"file://yelp_ontology.owl\")\n",
    "    ontology.load()\n",
    "\n",
    "    windows = [5, 7, 9, 11]\n",
    "\n",
    "    for w in windows:\n",
    "\n",
    "      sentences_window_file_name = './Austin-sb/w05/Sentences distance/austin-sl-sentences-n-itdl-'+str(n)+'bin-wgt'+str(wgt)+'-wnd'+str(w)+'-d.csv'\n",
    "      #sentences_window_file_name = 'austin-sl-sentences-n-itdl-'+str(n)+'bin-wgt'+str(wgt)+'-wnd'+str(w)+'-d.csv'\n",
    "      sentences = pd.read_csv(sentences_window_file_name)\n",
    "      \n",
    "      layers = [1, 2, 3]\n",
    "      for l in layers:\n",
    "\n",
    "        layer_sentences = getSentencesFromLayer(ontology, sentences, l)\n",
    "\n",
    "        #Se há dados\n",
    "        if(len(layer_sentences) > 0):\n",
    "          layer_sentences_file_name = './Austin-sb/w05/Sentences distance/austin-sl-sentences-n-itdl-'+str(n)+'bin-wgt'+str(wgt)+'-wnd'+str(w)+'-l'+str(l)+'-d.csv'    \n",
    "          #layer_sentences_file_name = 'austin-sl-sentences-n-itdl-'+str(n)+'bin-wgt'+str(wgt)+'-wnd'+str(w)+'-l'+str(l)+'-d.csv'   \n",
    "          layer_sentences.to_csv(layer_sentences_file_name, index=False)\n",
    "                      \n",
    "  except Exception as e:\n",
    "    e\n",
    "    #print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hb1p98wtzoRT"
   },
   "outputs": [],
   "source": [
    "def getTuplesFromLayer(ontology, tuples, layer):\n",
    "\n",
    "  layer_tuples = pd.DataFrame(columns=tuples.columns.to_list())\n",
    "  for id, row in tuples.iterrows():\n",
    "\n",
    "    centerPoI = row['center_poi'].replace(' ', '_')\n",
    "    contextPoI = row['context_poi'].replace(' ', '_')\n",
    "\n",
    "    try:\n",
    "\n",
    "      centerPoILayer = getClassLevel(ontology, centerPoI)\n",
    "      contextPoILayer = getClassLevel(ontology, contextPoI)\n",
    "\n",
    "      \n",
    "      if (centerPoILayer == contextPoILayer and layer in centerPoILayer):\n",
    "\n",
    "        layer_tuples = layer_tuples.append(pd.DataFrame(\n",
    "            [tuples.iloc[id].tolist()],\n",
    "            columns=tuples.columns.to_list()), ignore_index = True\n",
    "        )\n",
    "    except Exception as e:\n",
    "      e\n",
    "      #print(str(e))\n",
    "          \n",
    "  return layer_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Xz3COYELDVRk"
   },
   "outputs": [],
   "source": [
    "def getSentencesFromLayer(ontology, sentences, layer):\n",
    "\n",
    "  layer_sentences = pd.DataFrame(columns=sentences.columns.to_list())\n",
    "\n",
    "  for id in range(0, len(sentences)):\n",
    "    pois = sentences.iloc[id][sentences.iloc[id].values != 'Sentinel'].to_list()\n",
    "\n",
    "    centerPoI = pois[0].replace(' ', '_')\n",
    "    contextPoI = pois[1].replace(' ', '_')\n",
    "\n",
    "    try:\n",
    "\n",
    "      centerPoILayer = getClassLevel(ontology, centerPoI)\n",
    "      contextPoILayer = getClassLevel(ontology, contextPoI)\n",
    "\n",
    "      \n",
    "      if (centerPoILayer == contextPoILayer and layer in centerPoILayer):\n",
    "\n",
    "        layer_sentences = layer_sentences.append(pd.DataFrame(\n",
    "            [sentences.iloc[id].tolist()],\n",
    "            columns=sentences.columns.to_list()), ignore_index = True\n",
    "        )\n",
    "    except Exception as e:\n",
    "      e\n",
    "          \n",
    "  return layer_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xjos5bsgQgqh"
   },
   "outputs": [],
   "source": [
    "#TODO transformar no PAI para comparação\n",
    "def getRankingHitFromLevel(ontology, ranking, layer):\n",
    "  layer_ranking = pd.DataFrame(columns=ranking.columns.to_list())\n",
    "\n",
    "  for id, row in ranking.iterrows():\n",
    "\n",
    "    type1 = row['Type1'].replace(' ', '_')\n",
    "    type2 = row['Type2'].replace(' ', '_')\n",
    "\n",
    "    try:\n",
    "\n",
    "      type1Layer = getClassLevel(ontology, type1)\n",
    "      type2Layer = getClassLevel(ontology, type2)\n",
    "\n",
    "      print(type1, type1Layer, type2, type2Layer)\n",
    "      \n",
    "      if (type1Layer == type2Layer and layer in type1Layer):\n",
    "\n",
    "        layer_ranking = layer_ranking.append(pd.DataFrame(\n",
    "            [ranking.iloc[id].tolist()],\n",
    "            columns=ranking.columns.to_list()), ignore_index = True\n",
    "        )\n",
    "    except Exception as e:\n",
    "      e\n",
    "          \n",
    "  return layer_ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "m1dtTcXTS9MC"
   },
   "outputs": [],
   "source": [
    "def updateRankingHitByLevel(ontology, ranking):\n",
    "\n",
    "  layer_ranking = pd.DataFrame(columns=ranking.columns.to_list())\n",
    "  for id, row in ranking.iterrows():\n",
    "\n",
    "    type1 = row['Type1'].replace(' ', '_')\n",
    "    type2 = row['Type2'].replace(' ', '_')\n",
    "\n",
    "    try:\n",
    "\n",
    "      type1Layer = getClassLevel(ontology, type1)\n",
    "      type2Layer = getClassLevel(ontology, type2)\n",
    "\n",
    "        \n",
    "      type1Layer = type1Layer[0]\n",
    "      type2Layer = type2Layer[0]\n",
    "\n",
    "      #Obtendo relação com os pais, até se igualar ao poi em comparação\n",
    "\n",
    "      if (type1Layer != type2Layer):\n",
    "        if (type1Layer > type2Layer):\n",
    "\n",
    "            parentsType1 = getClassParents(ontology, type1)\n",
    "            for parent in parentsType1:\n",
    "\n",
    "              \n",
    "              parentLevel = getClassLevel(ontology, parent[0])\n",
    "              print(type2, type2Layer, type1, type1Layer, parent[0], parentLevel[0])\n",
    "\n",
    "              if (parentLevel[0] == type2Layer):\n",
    "                parent = parent[0].replace('_', ' ')\n",
    "                layer_ranking = layer_ranking.append(pd.DataFrame([[parent, row['Type2'], row['Human Mean(1-7)']]],\n",
    "                                                    columns=ranking.columns.to_list()), \n",
    "                                                    ignore_index = True\n",
    "                                                    )\n",
    "                break\n",
    "\n",
    "        #type2 > type1      \n",
    "        else:\n",
    "\n",
    "            parentsType2 = getClassParents(ontology, type2)\n",
    "            for parent in parentsType2:\n",
    "\n",
    "              parentLevel = getClassLevel(ontology, parent[0])\n",
    "              print(type1, type1Layer, type2, type2Layer, parent[0], parentLevel[0])\n",
    "\n",
    "              if (parentLevel[0] == type1Layer):\n",
    "                parent = parent[0].replace('_', ' ')\n",
    "                layer_ranking = layer_ranking.append(pd.DataFrame([[row['Type1'], parent, row['Human Mean(1-7)']]],\n",
    "                                                    columns=ranking.columns.to_list()), \n",
    "                                                    ignore_index = True\n",
    "                                                    )\n",
    "                break\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "\n",
    "  layer_ranking = pd.concat([ranking, layer_ranking])\n",
    "  return layer_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "clCSMzrRvV6F"
   },
   "outputs": [],
   "source": [
    "def updateBynaryHitByLevel(ontology, binary):\n",
    "\n",
    "  binary_df = pd.DataFrame(columns=['Type1', 'Count1', 'Type2', 'Count2', 'Type3', 'Count3'])\n",
    "\n",
    "  for key_i in binary:\n",
    "    \n",
    "    places = list(binary[key_i])\n",
    "    \n",
    "    binary_df = binary_df.append(pd.DataFrame([[places[0], binary[key_i][places[0]],\n",
    "                                    places[1], binary[key_i][places[1]],\n",
    "                                    places[2], binary[key_i][places[2]]]], columns=['Type1', 'Count1', 'Type2', 'Count2', 'Type3', 'Count3']),\n",
    "                                  ignore_index = True)\n",
    "\n",
    "    try:\n",
    "\n",
    "      level_place_1 = getClassLevel(ontology, places[0].replace(' ', '_'))[0]\n",
    "      level_place_2 = getClassLevel(ontology, places[1].replace(' ', '_'))[0]\n",
    "      level_place_3 = getClassLevel(ontology, places[2].replace(' ', '_'))[0]\n",
    "\n",
    "      higher_level = level_place_1\n",
    "      if (level_place_2 < higher_level):\n",
    "        higher_level = level_place_2\n",
    "      elif (level_place_3 < higher_level):\n",
    "        higher_level = level_place_3\n",
    "\n",
    "      #print(higher_level)\n",
    "      place_1 = places[0]\n",
    "      place_2 = places[1]\n",
    "      place_3 = places[2]\n",
    "\n",
    "    \n",
    "\n",
    "      #Ajustando o primeiro lugar\n",
    "      if (level_place_1 > higher_level):\n",
    "\n",
    "        place_1_parents = getClassParents(ontology, place_1.replace(' ', '_'))\n",
    "        for parent in place_1_parents:\n",
    "          parent_level = getClassLevel(ontology, parent[0])\n",
    "\n",
    "          if (parent_level[0] == higher_level):\n",
    "              place_1 = parent[0].replace('_', ' ')\n",
    "              break\n",
    "\n",
    "      \n",
    "      #Ajustando o segundo lugar\n",
    "      if (level_place_2 > higher_level):\n",
    "\n",
    "        place_2_parents = getClassParents(ontology, place_2.replace(' ', '_'))\n",
    "        for parent in place_2_parents:\n",
    "          parent_level = getClassLevel(ontology, parent[0])\n",
    "\n",
    "          if (parent_level[0] == higher_level):\n",
    "              place_2 = parent[0].replace('_', ' ')\n",
    "              break\n",
    "\n",
    "      #Ajustando o segundo lugar\n",
    "      if (level_place_3 > higher_level):\n",
    "\n",
    "        place_3_parents = getClassParents(ontology, place_3.replace(' ', '_'))\n",
    "        for parent in place_3_parents:\n",
    "          parent_level = getClassLevel(ontology, parent[0])\n",
    "\n",
    "          if (parent_level[0] == higher_level):\n",
    "              place_3 = parent[0].replace('_', ' ')\n",
    "              break\n",
    "\n",
    "      #print(places[0], places[1], places[2])\n",
    "      #print(place_1, place_2, place_3)\n",
    "      #print()\n",
    "\n",
    "      binary_df = binary_df.append(pd.DataFrame([[place_1, binary[key_i][places[0]],\n",
    "                                    place_2, binary[key_i][places[1]],\n",
    "                                    place_3, binary[key_i][places[2]]]], columns=['Type1', 'Count1', 'Type2', 'Count2', 'Type3', 'Count3']),\n",
    "                                  ignore_index = True)\n",
    "      #dic[key_i] = {place_1: binary[key_i][places[0]], place_2: binary[key_i][places[1]], place_3: binary[key_i][places[2]]}\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "\n",
    "  return binary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QFf_CbFywRKW"
   },
   "outputs": [],
   "source": [
    "def getRankingHitFromLevel(ontology, binary_df, layer):\n",
    "\n",
    "  binary_layer = pd.DataFrame(columns=['Type1', 'Count1', 'Type2', 'Count2', 'Type3', 'Count3'])\n",
    "\n",
    "  for id, row in binary_df.iterrows():\n",
    "\n",
    "    type1 = row['Type1'].replace(' ', '_')\n",
    "    type2 = row['Type2'].replace(' ', '_')\n",
    "    type3 = row['Type3'].replace(' ', '_')\n",
    "\n",
    "    try:\n",
    "\n",
    "      type1Layer = getClassLevel(ontology, type1)\n",
    "      type2Layer = getClassLevel(ontology, type2)\n",
    "      type3Layer = getClassLevel(ontology, type3)\n",
    "\n",
    "      #print(type1, type1Layer, type2, type2Layer)\n",
    "      \n",
    "      if (type1Layer == type2Layer and type3Layer == type1Layer and layer in type1Layer):\n",
    "\n",
    "        binary_layer = binary_layer.append(pd.DataFrame(\n",
    "            [binary_df.iloc[id].tolist()],\n",
    "            columns=binary_df.columns.to_list()), ignore_index = True\n",
    "        )\n",
    "    except Exception as e:\n",
    "      e\n",
    "  \n",
    "  return binary_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv02HZpUO13C"
   },
   "source": [
    "## Ontologias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4mQeHGusCRyn"
   },
   "outputs": [],
   "source": [
    "# Recebe a ontologia e uma classe, e devolve os pais e os níveis dos pais daquela classe\n",
    "# Para cada pai de um nó, encontra o novo pai e conta a profundidade\n",
    "def getClassParents(ontology, class_name):\n",
    "  depth = 0\n",
    "  parents_list = []\n",
    "  parents_queue = []\n",
    "  local_parents = ontology.get_parents_of(ontology[class_name])\n",
    "\n",
    "  #parents_queue.append([local_parents, depth])\n",
    "  parents_queue.append([local_parents, depth+1])\n",
    "\n",
    "  while(len(parents_queue) > 0):\n",
    "    [parents, local_depth] = parents_queue.pop(0)\n",
    "    for parent in parents:\n",
    "      #print(parent)\n",
    "      parents_list.append((parent.name, local_depth+1))\n",
    "      local_parents = ontology.get_parents_of(ontology[parent.name])\n",
    "      parents_queue.append((local_parents, (local_depth+1)))\n",
    "\n",
    "  #Retorna lista sem valores duplicados\n",
    "  return list(set([i for i in parents_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5cAWTJ5uQj9q"
   },
   "outputs": [],
   "source": [
    "# Recebe a ontologia e uma classe, e devolve o nível em que aquela classe está\n",
    "# Para cada pai de um nó, encontra o novo pai e conta a profundidade\n",
    "def getClassLevel(ontology, class_name):\n",
    "  depthList = []\n",
    "  parents = getClassParents(ontology, class_name)\n",
    "  #print(parents)\n",
    "\n",
    "  for parent, depth in parents:\n",
    "    \n",
    "    try:\n",
    "      if(len(ontology.get_parents_of(ontology[parent])) == 0):\n",
    "          depthList.append(depth)\n",
    "          #depthList.append(depth+1)\n",
    "    except Exception as e:\n",
    "      e\n",
    "\n",
    "  finalDepth = list(set(depthList))\n",
    " \n",
    "  if(len(finalDepth) == 0):\n",
    "    #finalDepth.append(0)\n",
    "    finalDepth.append(1)\n",
    "  return finalDepth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r6014z6nygao"
   },
   "outputs": [],
   "source": [
    "# Recebe a ontologia e o nível de classes que se deseja devolver\n",
    "# Para cada pai de um nó, encontra o novo pai e conta a profundidade\n",
    "def getClassesFromLevel(ontology, level):\n",
    "  classes = list(ontology.classes())\n",
    "  levelClasses = list() #Classes desse nível\n",
    "  for classe in classes:\n",
    "    depths = getClassLevel(ontology, classe.name)\n",
    "    for depth in depths: \n",
    "      if (depth == level):\n",
    "        levelClasses.append(classe.name)\n",
    "\n",
    "  return levelClasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siJpcHjKf0IB"
   },
   "source": [
    "# Seleção de Camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYdFWk3J3c0G",
    "outputId": "bd0d6046-a839-4102-bf2a-a5e6c4b71316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  16\n",
      "Extracting tuples of bin:Extracting tuples of bin:Extracting tuples of bin:Extracting tuples of bin:    1302   ...... ...\n",
      "\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())\n",
    "\n",
    "wgt = 0.5\n",
    "\n",
    "# Step 1: Init multiprocessing.Pool()\n",
    "pool = mp.Pool(int(mp.cpu_count()/4))\n",
    "\n",
    "# Step 2: `pool.apply` the `howmany_within_range()`\n",
    "bins = range(0, 14)\n",
    "\n",
    "pool.starmap(extractTuplesLayerByBin, [(n, wgt) for n in bins])\n",
    "#pool.starmap(extractSentencesLayerByBin, [(n, wgt) for n in bins])\n",
    "\n",
    "# Step 3: Don't forget to close\n",
    "pool.close()\n",
    "print(\"Process finish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "YelpLayerSelector.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "1c960ebc558cb47a91b30b6a69e09ee33d8511507a0164b187e789d12f3a22a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('place2vec': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
